{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3aaa2d-6b9c-4bd9-a1b8-215eac181193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10 - Mean Reward 23.2\n",
      "Ep 20 - Mean Reward 15.3\n",
      "Ep 30 - Mean Reward 12.9\n",
      "Ep 40 - Mean Reward 17.1\n",
      "Ep 50 - Mean Reward 60.8\n",
      "Ep 60 - Mean Reward 85.3\n",
      "Ep 70 - Mean Reward 60.8\n",
      "Ep 80 - Mean Reward 68.3\n",
      "Ep 90 - Mean Reward 82.0\n",
      "Ep 100 - Mean Reward 90.1\n",
      "Ep 110 - Mean Reward 81.7\n",
      "Ep 120 - Mean Reward 81.7\n",
      "Ep 130 - Mean Reward 96.7\n",
      "Ep 140 - Mean Reward 94.3\n",
      "Ep 150 - Mean Reward 66.9\n",
      "Ep 160 - Mean Reward 85.4\n",
      "Ep 170 - Mean Reward 78.7\n",
      "Ep 180 - Mean Reward 87.2\n",
      "Ep 190 - Mean Reward 76.3\n",
      "Ep 200 - Mean Reward 81.0\n",
      "Ep 210 - Mean Reward 76.3\n",
      "Ep 220 - Mean Reward 55.2\n",
      "Ep 230 - Mean Reward 82.2\n",
      "Ep 240 - Mean Reward 60.9\n",
      "Ep 250 - Mean Reward 99.6\n",
      "Ep 260 - Mean Reward 100.0\n",
      "Ep 270 - Mean Reward 98.7\n",
      "Ep 280 - Mean Reward 47.1\n",
      "Ep 290 - Mean Reward 40.2\n",
      "Ep 300 - Mean Reward 24.8\n",
      "Ep 310 - Mean Reward 85.6\n",
      "Ep 320 - Mean Reward 56.3\n",
      "Ep 330 - Mean Reward 44.8\n",
      "Ep 340 - Mean Reward 83.3\n",
      "Ep 350 - Mean Reward 53.0\n",
      "Ep 360 - Mean Reward 71.6\n",
      "Ep 370 - Mean Reward 78.4\n",
      "Ep 380 - Mean Reward 56.1\n",
      "Ep 390 - Mean Reward 51.5\n",
      "Ep 400 - Mean Reward 65.3\n",
      "Ep 410 - Mean Reward 61.4\n",
      "Ep 420 - Mean Reward 86.2\n",
      "Ep 430 - Mean Reward 92.1\n",
      "Ep 440 - Mean Reward 60.1\n",
      "Ep 450 - Mean Reward 75.4\n",
      "Ep 460 - Mean Reward 57.9\n",
      "Ep 470 - Mean Reward 76.2\n",
      "Ep 480 - Mean Reward 67.2\n",
      "Ep 490 - Mean Reward 77.0\n",
      "Ep 500 - Mean Reward 57.8\n",
      "Ep 510 - Mean Reward 83.0\n",
      "Ep 520 - Mean Reward 77.3\n",
      "Ep 530 - Mean Reward 88.3\n",
      "Ep 540 - Mean Reward 97.3\n",
      "Ep 550 - Mean Reward 82.1\n",
      "Ep 560 - Mean Reward 64.1\n",
      "Ep 570 - Mean Reward 100.0\n",
      "Ep 580 - Mean Reward 98.4\n",
      "Ep 590 - Mean Reward 94.1\n",
      "Ep 600 - Mean Reward 86.5\n",
      "Ep 610 - Mean Reward 60.5\n",
      "Ep 620 - Mean Reward 90.3\n",
      "Ep 630 - Mean Reward 96.4\n",
      "Ep 640 - Mean Reward 93.6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from AgentRL.agents.deep_q_network import DQN\n",
    "from AgentRL.common.buffers.prioritised_buffer import prioritised_replay_buffer\n",
    "from AgentRL.common.buffers.standard_buffer import standard_replay_buffer\n",
    "\n",
    "import torch\n",
    "\n",
    "def train(env):\n",
    "\n",
    "    # Set the hyperparameters\n",
    "    training = True\n",
    "    render = False\n",
    "    display_freq = 10\n",
    "    episodes = 1000\n",
    "    timestep_limit = 100\n",
    "    seed = 1\n",
    "    \n",
    "    # intialise the environment\n",
    "    env = env\n",
    "\n",
    "    running_reward = []\n",
    "\n",
    "    env.seed(seed)\n",
    "\n",
    "    # initialise the agent\n",
    "    buffer = prioritised_replay_buffer(max_size=50_000, seed=seed)\n",
    "    # buffer = standard_replay_buffer(max_size=50_000, seed=seed)\n",
    "    agent = DQN(\n",
    "        state_dim=env.observation_space.shape[0], \n",
    "        action_num=env.action_space.n, \n",
    "        replay_buffer=buffer,\n",
    "\n",
    "        algorithm_type='duelling',\n",
    "        hidden_dim = 16,\n",
    "        learning_rate = 5e-4,\n",
    "        batch_size = 32,\n",
    "        gamma = 0.95,\n",
    "        \n",
    "        target_update_method = 'hard',\n",
    "        tau = 0.01, # for soft\n",
    "        target_update_freq = 20, # for hard\n",
    "        \n",
    "        exploration_method=\"noisy_network\",\n",
    "                \n",
    "        categorical = True,\n",
    "        v_range = (0, 200),\n",
    "        atom_size = 51,\n",
    "\n",
    "        seed = seed\n",
    "\n",
    "    )\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "\n",
    "        # reset the state\n",
    "        state, done = env.reset(), False\n",
    "        counter = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        # run the training loop\n",
    "        while not done:\n",
    "\n",
    "            action = agent.get_action(state=state.flatten())              \n",
    "            next_state, reward, done, info = env.step(action=action[0])\n",
    "\n",
    "            # render the environment\n",
    "            if render: \n",
    "                env.render(mode='close')\n",
    "\n",
    "            # update the reward total\n",
    "            episode_reward += reward\n",
    "\n",
    "\n",
    "            if training: \n",
    "\n",
    "                # push test samples to the replay buffer\n",
    "                agent.push(state=state, action=action,\n",
    "                            next_state=next_state, reward=reward/100, done=done)\n",
    "\n",
    "                agent.update()                       \n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            counter += 1\n",
    "\n",
    "            # terminate when episode limit is reached            \n",
    "            if counter >= timestep_limit:\n",
    "                done = True\n",
    "\n",
    "            # print the episode reward\n",
    "            if done: \n",
    "\n",
    "                # get reward mean\n",
    "                running_reward.append(episode_reward)\n",
    "\n",
    "                if ep % display_freq == 0:\n",
    "                    # print('Ep {} - Mean Reward {} Exploration {}'.format(ep, sum(running_reward) / display_freq, round(agent.policy.current_exploration, 2)))\n",
    "                    print('Ep {} - Mean Reward {}'.format(ep, sum(running_reward) / display_freq))\n",
    "                    running_reward = []  \n",
    "\n",
    "    # close the display\n",
    "    env.close()    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # get the environment    \n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    # run the program\n",
    "    try: \n",
    "        train(env)\n",
    "    \n",
    "    # shut the y window if interrupted\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
